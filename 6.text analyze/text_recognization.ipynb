{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-1-b6934ef8bfaf>, line 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-b6934ef8bfaf>\"\u001b[1;36m, line \u001b[1;32m20\u001b[0m\n\u001b[1;33m    for name in input_names:\u001b[0m\n\u001b[1;37m                            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "#_*_ 性别识别：启发式方法，姓名的最后几个字符可以界定性别特征 _*_#\n",
    "import random\n",
    "from nltk.corpus import names\n",
    "from nltk import NaiveBayesClassifier\n",
    "from nltk.classify import accuracy as nltk_accuracy\n",
    "\n",
    "# 提取输入单词的特征\n",
    "def gender_features(word, num_letters=2):\n",
    "    return {'feature': word[-num_letters:].lower()}\n",
    "\n",
    "# 提取标记\n",
    "labeled_names = ([(name, 'male') for name in names.words('male.txt')] +\n",
    "[(name, 'female') for name in names.words('female.txt')])\n",
    "\n",
    "# 生成随机种子数，并搅乱训练数据\n",
    "random.seed(7)\n",
    "random.shuffle(labeled_names)\n",
    "\n",
    "input_names = []\n",
    "for i in range(1,5):\n",
    "    print('\\nNumbers of letters :',i)\n",
    "    featuresets = [ (gender_features(n,i),gender) for (n, gender) in labeled_names]\n",
    "\n",
    "    train_set, test_set = featuresets[500:], featuresets[:500]\n",
    "    classifier = NaiveBayesClassifier.train(train_set)\n",
    "    print('\\nAccuracy =',str(100*nltk_accuracy(classifier, test_set)) + str('%'))\n",
    "\n",
    "    for name in input_names:\n",
    "        print(name,'-->',classifier.classify(gender_features(name, i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of training datapoints: 1600\n",
      "\n",
      "Number of testing datapoints: 400\n",
      "\n",
      "Accuracy of the classifier : 0.735\n",
      "\n",
      "Top 10 most information words:\n",
      "outstanding\n",
      "insulting\n",
      "vulnerable\n",
      "ludicrous\n",
      "uninvolving\n",
      "astounding\n",
      "avoids\n",
      "fascination\n",
      "affecting\n",
      "animators\n",
      "\n",
      "Predictions:\n",
      "\n",
      "Review: It is an amazing movie\n",
      "Predicted sentiment: Positive\n",
      "Probability: 0.61\n",
      "\n",
      "Review: This is a dull movie. I would never recommend it to anyone.\n",
      "Predicted sentiment: Negative\n",
      "Probability: 0.77\n",
      "\n",
      "Review: The cinematography is pretty great in this movie\n",
      "Predicted sentiment: Positive\n",
      "Probability: 0.67\n",
      "\n",
      "Review: The direction was terrible and the story was all over the place\n",
      "Predicted sentiment: Negative\n",
      "Probability: 0.63\n",
      "\n",
      "Review: outstanding insulting\n",
      "Predicted sentiment: Positive\n",
      "Probability: 0.5\n"
     ]
    }
   ],
   "source": [
    "#_*_ 情感分析：分析人对某个特定主题的看法（积极、消极、中性） _*_#\n",
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "# NLTK分类器的数据需要以字典格式存储\n",
    "def extract_features(word_list):\n",
    "    return dict([(word, True) for word in word_list])\n",
    "\n",
    "# load datasets\n",
    "positive_fileids = movie_reviews.fileids('pos')\n",
    "negative_fileids = movie_reviews.fileids('neg')\n",
    "\n",
    "# 区分积极与消极评论\n",
    "features_positive = [(extract_features(movie_reviews.words(fileids=[f])),'Positive')\n",
    "                    for f in positive_fileids]\n",
    "features_negative = [(extract_features(movie_reviews.words(fileids=[f])),'Negative') \n",
    "                     for f in negative_fileids]\n",
    "\n",
    "threshold_factor = 0.8\n",
    "threshold_positive = int(threshold_factor * len(features_positive))\n",
    "threshold_negative = int(threshold_factor * len(features_negative))\n",
    "\n",
    "# extract features\n",
    "features_train = features_positive[:threshold_positive] + features_negative[:threshold_negative]\n",
    "features_test = features_positive[threshold_positive:] + features_negative[threshold_negative:]\n",
    "\n",
    "print(\"\\nNumber of training datapoints:\",len(features_train))\n",
    "print('\\nNumber of testing datapoints:',len(features_test))\n",
    "\n",
    "# training and valuate\n",
    "classifier = NaiveBayesClassifier.train(features_train)\n",
    "print('\\nAccuracy of the classifier :',nltk.classify.util.accuracy(classifier, features_test))\n",
    "\n",
    "# print\n",
    "print('\\nTop 10 most information words:')\n",
    "for item in classifier.most_informative_features()[:10]:\n",
    "    print(item[0])\n",
    "    \n",
    "# predict\n",
    "input_reviews = [\n",
    "\"It is an amazing movie\",\n",
    "\"This is a dull movie. I would never recommend it to anyone.\",\n",
    "\"The cinematography is pretty great in this movie\",\n",
    "\"The direction was terrible and the story was all over the place\",\n",
    "\"outstanding insulting\"\n",
    "]\n",
    "\n",
    "print(\"\\nPredictions:\")\n",
    "for review in input_reviews:\n",
    "    print('\\nReview:',review)\n",
    "    probdist = classifier.prob_classify(extract_features(review.split()))\n",
    "    pred_sentiment = probdist.max()\n",
    "    print(\"Predicted sentiment:\",pred_sentiment)\n",
    "    print(\"Probability:\",round(probdist.prob(pred_sentiment), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(42 unique tokens: ['cryptographi', 'lot', 'spent', 'studi', 'time']...)\n",
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)], [(5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1)], [(13, 1), (14, 1), (15, 1), (16, 1)], [(17, 1), (18, 1), (19, 1), (20, 1)], [(8, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1)], [(28, 1), (29, 1), (30, 1), (31, 1)], [(8, 1), (9, 1), (32, 1), (33, 1), (34, 1), (35, 1)], [(8, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1)]]\n",
      "\n",
      "Most contributing words to the topics:\n",
      "\n",
      "Topic 0  =>  0.070*\"need\" + 0.050*\"order\" + 0.030*\"system\" + 0.030*\"understand\"\n",
      "\n",
      "Topic 1  =>  0.039*\"promot\" + 0.039*\"talent\" + 0.039*\"train\" + 0.039*\"younger\"\n"
     ]
    }
   ],
   "source": [
    "#_*_ 主题建模：识别一组文档的隐藏主题模式 _*_#\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from gensim import models, corpora\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# load data\n",
    "def load_data(input_file):\n",
    "    data = []\n",
    "    with open(input_file,'r') as f:\n",
    "        for line in f.readlines():\n",
    "            data.append(line[:-1])\n",
    "    \n",
    "    return data\n",
    "\n",
    "# 预处理文本类：从输入文本中提取相关的特征\n",
    "class Preprocessor(object):\n",
    "    def __init__(self):\n",
    "        # 默认使用正则表达式解析器\n",
    "        self.tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        \n",
    "        # stop words\n",
    "        self.stop_words_english = stopwords.words('english')\n",
    "        \n",
    "        # stemmer\n",
    "        self.stemmer = SnowballStemmer('english')\n",
    "        \n",
    "    # 处理函数：标记解析、停用词去除、词干还原\n",
    "    def process(self, input_text):\n",
    "        # 标记解析\n",
    "        tokens = self.tokenizer.tokenize(input_text.lower())\n",
    "        # 移除停用词\n",
    "        tokens_stopwords = [x for x in tokens if not x in self.stop_words_english]\n",
    "        # 词干提取\n",
    "        tokens_stemmed = [self.stemmer.stem(x) for x in tokens_stopwords]\n",
    "        \n",
    "        return tokens_stemmed\n",
    "    \n",
    "input_file = 'data_topic_modeling.txt'\n",
    "data = load_data(input_file)\n",
    "preprocessor = Preprocessor()\n",
    "\n",
    "# 处理文本，并提取已处理好的标记\n",
    "processed_tokens = [preprocessor.process(x) for x in data]\n",
    "\n",
    "# 创建基于标记文档的字典\n",
    "dict_tokens = corpora.Dictionary(processed_tokens)\n",
    "print(dict_tokens)\n",
    "\n",
    "# 字典转为文档-词矩阵\n",
    "corpus = [dict_tokens.doc2bow(text) for text in processed_tokens]\n",
    "print(corpus)\n",
    "\n",
    "# 隐Dirichlet分布(LDA)做主题建模\n",
    "num_topics = 2\n",
    "num_words = 4\n",
    "# fit\n",
    "ldamodel = models.ldamodel.LdaModel(corpus, num_topics=num_topics, id2word=dict_tokens, passes=25)\n",
    "\n",
    "print('\\nMost contributing words to the topics:')\n",
    "for item in ldamodel.print_topics(num_topics=num_topics, num_words=num_words):\n",
    "    print('\\nTopic',item[0],' => ',item[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "主题建模，通过识别文档中最有意义、最能表征主题的词来实现主题分类\n",
    "\n",
    "文本解析(分词) -> 去除停用词(除噪) -> 词干还原(归一化)\n",
    "\n",
    "LDA是生成主题的模型：找出所有主题，再生成给定主题的文档\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
