{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "目标识别系统：从每张图中提取特征向量，与其识别标志（视觉码本）做匹配\n",
    "视觉码本：一种图像的描述字典，用向量量化法将图像的特征点聚类，并得出中心点\n",
    "'''\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "class StarFeatureDetector(object):\n",
    "    def __init__(self):\n",
    "        self.detector = cv2.xfeatures2d.StarDetector_create()\n",
    "\n",
    "    def detect(self, img):\n",
    "        return self.detector.detect(img)\n",
    "\n",
    "def load_training_data(input_folder):\n",
    "    training_data = []\n",
    "\n",
    "    if not os.path.isdir(input_folder):\n",
    "        raise IOError(\"The folder \" + input_folder + \" doesn't exist\")\n",
    "        \n",
    "    for root, dirs, files in os.walk(input_folder):\n",
    "        for filename in (x for x in files if x.endswith('.jpg')):\n",
    "            filepath = os.path.join(root, filename)\n",
    "            object_class = filepath.split('/')[-2]\n",
    "            training_data.append({'object_class': object_class, \n",
    "                'image_path': filepath})\n",
    "                    \n",
    "    return training_data\n",
    "\n",
    "class FeatureBuilder(object):\n",
    "    def extract_features(self, img):\n",
    "        keypoints = StarFeatureDetector().detect(img)\n",
    "        keypoints, feature_vectors = compute_sift_features(img, keypoints)\n",
    "        return feature_vectors\n",
    "\n",
    "    def get_codewords(self, input_map, scaling_size, max_samples=12):\n",
    "        keypoints_all = []\n",
    "        \n",
    "        count = 0\n",
    "        cur_class = ''\n",
    "        for item in input_map:\n",
    "            if count >= max_samples:\n",
    "                if cur_class != item['object_class']:\n",
    "                    count = 0\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            count += 1\n",
    "\n",
    "            if count == max_samples:\n",
    "                print(\"Built centroids for\", item['object_class'])\n",
    "\n",
    "            cur_class = item['object_class']\n",
    "            img = cv2.imread(item['image_path'])\n",
    "            img = resize_image(img, scaling_size)\n",
    "\n",
    "            num_dims = 128\n",
    "            feature_vectors = self.extract_features(img)\n",
    "            keypoints_all.extend(feature_vectors) \n",
    "\n",
    "        kmeans, centroids = BagOfWords().cluster(keypoints_all)\n",
    "        return kmeans, centroids\n",
    "\n",
    "class BagOfWords(object):\n",
    "    def __init__(self, num_clusters=32):\n",
    "        self.num_dims = 128\n",
    "        self.num_clusters = num_clusters\n",
    "        self.num_retries = 10\n",
    "\n",
    "    def cluster(self, datapoints):\n",
    "        kmeans = KMeans(self.num_clusters, \n",
    "                        n_init=max(self.num_retries, 1),\n",
    "                        max_iter=10, tol=1.0)\n",
    "\n",
    "        res = kmeans.fit(datapoints)\n",
    "        centroids = res.cluster_centers_\n",
    "        return kmeans, centroids\n",
    "\n",
    "    def normalize(self, input_data):\n",
    "        sum_input = np.sum(input_data)\n",
    "\n",
    "        if sum_input > 0:\n",
    "            return input_data / sum_input\n",
    "        else:\n",
    "            return input_data\n",
    "\n",
    "    def construct_feature(self, img, kmeans, centroids):\n",
    "        keypoints = StarFeatureDetector().detect(img)\n",
    "        keypoints, feature_vectors = compute_sift_features(img, keypoints)\n",
    "        labels = kmeans.predict(feature_vectors)\n",
    "        feature_vector = np.zeros(self.num_clusters)\n",
    "\n",
    "        for i, item in enumerate(feature_vectors):\n",
    "            feature_vector[labels[i]] += 1\n",
    "\n",
    "        feature_vector_img = np.reshape(feature_vector, \n",
    "                ((1, feature_vector.shape[0])))\n",
    "        return self.normalize(feature_vector_img)\n",
    "\n",
    "# Extract features from the input images and \n",
    "# map them to the corresponding object classes\n",
    "def get_feature_map(input_map, kmeans, centroids, scaling_size):\n",
    "    feature_map = []\n",
    "     \n",
    "    for item in input_map:\n",
    "        temp_dict = {}\n",
    "        temp_dict['object_class'] = item['object_class']\n",
    "    \n",
    "        print(\"Extracting features for\", item['image_path'])\n",
    "        img = cv2.imread(item['image_path'])\n",
    "        img = resize_image(img, scaling_size)\n",
    "\n",
    "        temp_dict['feature_vector'] = BagOfWords().construct_feature(\n",
    "                    img, kmeans, centroids)\n",
    "\n",
    "        if temp_dict['feature_vector'] is not None:\n",
    "            feature_map.append(temp_dict)\n",
    "\n",
    "    return feature_map\n",
    "\n",
    "# Extract SIFT features\n",
    "def compute_sift_features(img, keypoints):\n",
    "    if img is None:\n",
    "        raise TypeError('Invalid input image')\n",
    "\n",
    "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    keypoints, descriptors = cv2.xfeatures2d.SIFT_create().compute(img_gray, keypoints)\n",
    "    return keypoints, descriptors\n",
    "\n",
    "# Resize the shorter dimension to 'new_size' \n",
    "# while maintaining the aspect ratio\n",
    "def resize_image(input_img, new_size):\n",
    "    h, w = input_img.shape[:2]\n",
    "    scaling_factor = new_size / float(h)\n",
    "\n",
    "    if w < h:\n",
    "        scaling_factor = new_size / float(w)\n",
    "\n",
    "    new_shape = (int(w * scaling_factor), int(h * scaling_factor))\n",
    "    return cv2.resize(input_img, new_shape) \n",
    "\n",
    "if __name__=='__main__':\n",
    "    data_folder = './car_img'\n",
    "    scaling_size = 8\n",
    "    \n",
    "    # Load the training data\n",
    "    training_data = load_training_data(data_folder)\n",
    "\n",
    "    # Build the visual codebook\n",
    "    print(\"====== Building visual codebook ======\")\n",
    "    kmeans, centroids = FeatureBuilder().get_codewords(training_data, scaling_size)\n",
    "    if args.codebook_file:\n",
    "        with open(args.codebook_file, 'w') as f:\n",
    "            pickle.dump((kmeans, centroids), f)\n",
    "    \n",
    "    # Extract features from input images\n",
    "    print(\"\\n====== Building the feature map ======\")\n",
    "    feature_map = get_feature_map(training_data, kmeans, centroids, scaling_size)\n",
    "    if args.feature_map_file:\n",
    "        with open(args.feature_map_file, 'w') as f:\n",
    "            pickle.dump(feature_map, f)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用极端随机森林训练图像分类器 ：基于图像的特征构建一组决策树，实现图像的分类识别\n",
    "import pickle\n",
    "import numpy as np\n",
    "import sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# ERF classifier\n",
    "class ERFTrainer(object):\n",
    "    def __init__(self, X, label_words):\n",
    "        self.le = preprocessing.LabelEncoder()\n",
    "        self.clf = ExtraTreesClassifier(n_estimators=100, max_depth=16, random_state=0)\n",
    "        \n",
    "        # 对标签编码，并训练分类器\n",
    "        y = self.encode_labels(label_words)\n",
    "        self.clf.fit(np.asarray(X), y)\n",
    "        \n",
    "    def encode_labels(self, label_words):\n",
    "        self.le.fit(label_words)\n",
    "        return np.array(self.le.transform(label_words), dtype=np.float32)\n",
    "    \n",
    "    # classifier\n",
    "    def classify(self, X):\n",
    "        label_nums = self.clf.predict(np.asarray(X))\n",
    "        label_words = self.le.inverse_transform([int(x) for x in label_nums])\n",
    "        return label_words\n",
    "    \n",
    "feature_map_file = '.pkl'\n",
    "model_file = '.pkl'\n",
    "\n",
    "with open(feature_map_file,'r') as f:\n",
    "    feature_map = pickle.loads(f.read())\n",
    "\n",
    "# 提取特征向量和标记\n",
    "label_words = [x['object_class'] for x in feature_map]\n",
    "dim_size = feature_map[0]['feature_vector'].shape[1]\n",
    "X = [np.reshape(x['feature_vector'], (dim_size,)) for x in feature_map]\n",
    "\n",
    "# training\n",
    "erf = ERFTrainer(X, label_words)\n",
    "\n",
    "with open(model_file, 'w') as f:\n",
    "    pickle.dump(erf, f)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
